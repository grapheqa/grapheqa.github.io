<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="GraphEQA is a novel approach that utilizes real-time 3D metric-semantic scene graphs (3DSG) and task relevant images as multi-modal memory for grounding Vision-Language Models (VLMs) to perform EQA tasks in unseen environments.">
  <meta name="keywords" content="EQA, embodied question answering, scene graph">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>GraphEQA: Using 3D Semantic Scene Graphs for Real-time Embodied Question Answering</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation"> -->
<!--   <div class="navbar-brand"> -->
<!--     <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false"> -->
<!--       <span aria-hidden="true"></span> -->
<!--       <span aria-hidden="true"></span> -->
<!--       <span aria-hidden="true"></span> -->
<!--     </a> -->
<!--   </div> -->
<!--   <div class="navbar-menu"> -->
<!--     <div class="navbar-start" style="flex-grow: 1; justify-content: center;"> -->
<!--       <a class="navbar-item" href="https://keunhong.com"> -->
<!--       <span class="icon"> -->
<!--           <i class="fas fa-home"></i> -->
<!--       </span> -->
<!--       </a> -->

<!--       <div class="navbar-item has-dropdown is-hoverable"> -->
<!--         <a class="navbar-link"> -->
<!--           More Research -->
<!--         </a> -->
<!--         <div class="navbar-dropdown"> -->
<!--           <a class="navbar-item" href="https://hypernerf.github.io"> -->
<!--             HyperNeRF -->
<!--           </a> -->
<!--           <a class="navbar-item" href="https://nerfies.github.io"> -->
<!--             Nerfies -->
<!--           </a> -->
<!--           <a class="navbar-item" href="https://latentfusion.github.io"> -->
<!--             LatentFusion -->
<!--           </a> -->
<!--           <a class="navbar-item" href="https://photoshape.github.io"> -->
<!--             PhotoShape -->
<!--           </a> -->
<!--         </div> -->
<!--       </div> -->
<!--     </div> -->

<!--   </div> -->
<!-- </nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">GraphEQA: Using 3D Semantic Scene Graphs for Real-time Embodied Question Answering</h1>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
<!--               <span class="link-block">
                <a href="https://www.google.com"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
<!--               <span class="link-block">
                <a href="https://www.google.com"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/grapheqa/graph_eqa"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (coming soon)</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay controls muted loop playsinline height="10%">
        <source src="./static/videos/grapheqa_bluepan_norobot_fast_actual.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">GraphEQA</span> deployed on the <a href="https://hello-robot.com/">Hello Robot</a> Stretch RE2 platform in a home environment. 
        <br> Question: <i>"Is there a blue pan on the stove?"</i> 
        <br> 
        <span style="text-align: left; display: block;">
          Left panel: Robot navigating a home, while planning and mapping the environment in real-time; captured via an externally mounted camera. 
          <br> 
          Right panel: Metric-semantic 3D mesh and scene graph construction from <a href="https://github.com/MIT-SPARK/Hydra?tab=readme-ov-file">Hydra</a>. TSDF-based 2D occupancy map where white nodes represent explored areas, red nodes are obstacles, blue nodes are clustered frontiers. Green node shows the target location chosen by the planner. 
          <br> 
          Right panel inset: Video feed from the robot head camera.
      </span>
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="method-image">
          <img src="./static/images/main_figure_02.png" alt="A depiction of GraphEQA in simulation." />
        </div>
        <div class="content has-text-justified">
          <p>
            In Embodied Question Answering (EQA), agents must explore and develop a semantic understanding of an unseen environment to answer a situated question with confidence. This problem remains challenging in robotics, due to the difficulties in obtaining useful semantic representations, updating these representations online, and leveraging prior world knowledge for efficient planning and exploration. To address these limitations, we propose GraphEQA, a novel approach that utilizes real-time 3D metric-semantic scene graphs (3DSGs) and task relevant images as multi-modal memory for grounding Vision-Language Models (VLMs) to perform EQA tasks in unseen environments. We employ a hierarchical planning approach that exploits the hierarchical nature of 3DSGs for structured planning and semantics-guided exploration. We evaluate GraphEQA in simulation on two benchmark datasets, HM-EQA and OpenEQA, and demonstrate that it outperforms key baselines by completing EQA tasks with higher success rates and fewer planning steps. We further demonstrate GraphEQA in multiple real-world home and office environments.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered"> -->
    <!--   <div class="column is-four-fifths"> -->
    <!--     <h2 class="title is-3">Video</h2> -->
    <!--     <div class="publication-video"> -->
    <!--       <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0" -->
    <!--               frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
    <!--     </div> -->
    <!--   </div> -->
    <!-- </div> -->
    <!--/ Paper video. -->

    <!-- Method image. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="method-image">
          <img src="./static/images/method.png" alt="The overall GraphEQA method." />
        </div>
        <div class="content has-text-justified">
          <p>
            Overall GraphEQA architecture. As the agent explores the environment, it used its sensor data (RGBD images, semantic map, camera poses and intrinsics) to construct a 3D metric-semantic hierarchical scene graph (3DSG) as well as a 2D occupancy map for frontier selection in real time. The constructed 3DSG is then enriched with semantic room labels and semantically-enriched frontiers. From the set of images collected during each trajectory execution, a task-relevant subset is selected, called the task-relevant visual memory. A VLM-based planner takes as input the enriched scene graph, task-relevant visual memory, a history of past states and actions, and the embodied question and outputs the answer, its confidence in the selected answer, and the next step it needs to take in the environment. If the VLM agent is confident in its answer, the episode is terminated, else the proposed action is executed in the environment and the process repeats.
          </p>
        </div>
      </div>
    </div>
    <!--/ Method image. -->

     <!-- VLM Planner image. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Hierarchical VLM Planner Architecture</h2>
        <div class="method-image">
          <img src="./static/images/vlm_planner_architecture.png" width="60%" alt="The overall GraphEQA method." />
        </div>
        <div class="content has-text-justified">
          <p>
            The Hierarchical Vision-Language planner takes as input the question, enriched scene graph, task-relevant visual memory, current state of the robot (position and room) and a history of past states, actions, answers and confidence values. The planner chooses the next <i>Goto_Object_node</i> action hierarchically by first selecting the room node and then the object node. The <i>Goto_Frontier_node</i> action is chosen based on the object nodes connected to the frontier via edges in the scene graph. The planner is asked to output a brief reasoning behind choosing each action. The planner also outputs an answer, confidence in its answer, reasoning behind the answer and confidence, the next action, a brief description of the scene graph and the visual memory.
          </p>
        </div>
      </div>
    </div>
    <!--/ VLM Planner image. -->

    <!-- Results image. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experimental Results</h2>
        <div class="method-image">
          <img src="./static/images/grapheqa_baselines_table.png" width="100%" alt="The results." />
        </div>
        <div class="content has-text-justified">
          <p>
            We compare the performance of GraphEQA against competitive baselines in simulation in the Habitat-Sim environment on the HM-EQA and OpenEQA datasets and in the real-world in two unique home environments. 
            The above table shows comparison to baselines in simulation using metrics: average success rate (%), average number of VLM planning steps, and average trajectory length L<sub>&tau;</sub>.
            We compare against several baselines and focus on methods that employ VLM-based planners for solving EQA or object goal navigation tasks. We compare against a strong baseline, <a href="https://explore-eqa.github.io/">Explore-EQA</a>, which calibrates Prismatic-VLM to answer embodied questions confidently while maintaining a 2D semantic memory and
            using prompted images to guide exploration. Note that ExploreEQA is always executed for a prespecified maximum number of steps, with the highest confidence step chosen to answer the question,
            while GraphEQA terminates based on a confidence threshold. We implement additional variants of
            ExploreEQA with newer foundation models --- e.g., GPT4o, Llama 4 Maverick, and Gemini 2.5 Pro --- for fair comparison with respective VLM variants of GraphEQA.
            We also compare against SayPlanVision, a modified version of <a href="https://sayplan.github.io/">SayPlan</a>, which in addition to the full scene graph also has access to task-relevant visual memory. 
            We observe that GraphEQA has higher success rate as compared to Explore-EQA and Explore-EQA-GPT4o, without the need to build an explicit 2D semantic task-specific memory. 
          </p>
          <p>
            Compared to Explore-EQA, our method completes the task in significantly lower planning steps and navigates the environment more efficiently (lower trajectory length).
            GraphEQA outperforms SayPlanVision, without needing access to the complete scene graph.
            Qualitatively, we observe that given access to the complete scene graph, SayPlanVision is overconfident about its choice of object node actions, leading to shorter trajectory lengths in successful cases, but also to increased failure cases.
            Table 1 shows simulation results comparing GraphEQA to the above baselines on the HM-EQA and OpenEQA datasets.
            Overall, GraphEQA outperforms all other baselines. Compared to Explore-EQA, our method completes tasks in significantly fewer planning steps and with lower trajectory length, indicating more efficient navigation.
            We also observe that the GPT and Llama variants of Explore-EQA have lower success rates than Explore-EQA, with qualitative results indicating overconfidence in VLMs' predictions, leading to terminating episodes before exploring sufficiently.
            We note that Explore-EQA's Gemini variant performs better than Explore-EQA, likely due to the inherent spatial reasoning capabilities of Gemini 2.5 Pro. 
            GraphEQA outperforms SayPlanVision even though SayPlanVision has access to the complete scene graph.
          </p>
        </div>
      </div>
    </div>
    <!--/ Results image. -->
  </div>
</section>

<!-- <div style="display: flex; justify-content: center;">
  <h2 class="title is-3">Additional Real-world Experiments for Home (a)</h2>
</div>
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="40%">
            <source src="./static/videos/grapheqa_door_question_compressed_cropped_fast.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-cushions">
          <video poster="" id="cushions" autoplay controls muted loop playsinline height="40%">
            <source src="./static/videos/grapheqa_couch_cushions_question_v2_compressed_cropped_fast.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-handbag">
          <video poster="" id="handbag" autoplay controls muted loop playsinline height="40%">
            <source src="./static/videos/grapheqa_handbag_question_v2_compressed_cropped_fast.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-trashcan">
          <video poster="" id="trashcan" autoplay controls muted loop playsinline height="40%">
            <source src="./static/videos/grapheqa_trashcan_question_v2_compressed_cropped_fast.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->

<!-- <div style="display: flex; justify-content: center;">
  <h2 class="title is-3">Additional Real-world Experiments for Home (b)</h2>
</div>
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay controls muted loop playsinline height="10%">
        <source src="./static/videos/grapheqa1_small.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">GraphEQA</span> deployed on the <a href="https://hello-robot.com/">Hello Robot</a> Stretch RE2 platform in Home Environment (b). 
        <br> Question: <i>"What color is the dehumidifier machine?" A) Blue, B) White, C) Black</i> 
        <br> 
        <span style="text-align: left; display: block;">
          Top panel: Robot navigating a home, while planning and mapping the environment in real-time; captured via an externally mounted camera. 
          <br> 
          Bottom left panel: Metric-semantic 3D mesh and scene graph construction from <a href="https://github.com/MIT-SPARK/Hydra?tab=readme-ov-file">Hydra</a>. TSDF-based 2D occupancy map where white nodes represent explored areas, red nodes are obstacles, blue nodes are clustered frontiers. Green node shows the target location chosen by the planner. 
          <br> 
          Bottom panel inset: Video feed from the robot head camera.
      </span>
      </h2>
    </div>
  </div>
</section> -->

<div style="display: flex; justify-content: center;">
  <h2 class="title is-3">Additional Real-world Experiments for Home (b)</h2>
</div>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div style="display: flex; flex-wrap: wrap; gap: 1rem; justify-content: center;">
        
        <!-- Video 1 -->
        <div style="flex: 1 1 45%; max-width: 45%;">
          <video autoplay controls muted loop playsinline style="width: 100%;">
            <source src="./static/videos/grapheqa1_small.mp4" type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            <span class="dnerf">GraphEQA</span> on Hello Robot Stretch RE2 (Home b).
            <br> Q: <i>"What color is the dehumidifier?" A) Blue, B) White, C) Black</i> 
            <br> Mapping, planning, and robot view.
          </h2>
        </div>

        <!-- Video 2 -->
        <div style="flex: 1 1 45%; max-width: 45%;">
          <video autoplay controls muted loop playsinline style="width: 100%;">
            <source src="./static/videos/grapheqa1_small.mp4" type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            <span class="dnerf">GraphEQA</span> on Hello Robot Stretch RE2 (Home c).
            <br> Q: <i>"Where is the orange trash bin?" A) Bathroom, B) Kitchen, C) Living room</i> 
            <br> Alternative environment deployment.
          </h2>
        </div>

      </div>
    </div>
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">

    <!-- <div class="columns is-centered"> -->

    <!--   <\!-- Visual Effects. -\-> -->
    <!--   <div class="column"> -->
    <!--     <div class="content"> -->
    <!--       <h2 class="title is-3">Visual Effects</h2> -->
    <!--       <p> -->
    <!--         Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect -->
    <!--         would be impossible without nerfies since it would require going through a wall. -->
    <!--       </p> -->
    <!--       <video id="dollyzoom" autoplay controls muted loop playsinline height="100%"> -->
    <!--         <source src="./static/videos/dollyzoom-stacked.mp4" -->
    <!--                 type="video/mp4"> -->
    <!--       </video> -->
    <!--     </div> -->
    <!--   </div> -->
    <!--   <\!--/ Visual Effects. -\-> -->

    <!--   <\!-- Matting. -\-> -->
    <!--   <div class="column"> -->
    <!--     <h2 class="title is-3">Matting</h2> -->
    <!--     <div class="columns is-centered"> -->
    <!--       <div class="column content"> -->
    <!--         <p> -->
    <!--           As a byproduct of our method, we can also solve the matting problem by ignoring -->
    <!--           samples that fall outside of a bounding box during rendering. -->
    <!--         </p> -->
    <!--         <video id="matting-video" controls playsinline height="100%"> -->
    <!--           <source src="./static/videos/matting.mp4" -->
    <!--                   type="video/mp4"> -->
    <!--         </video> -->
    <!--       </div> -->

    <!--     </div> -->
    <!--   </div> -->
    <!-- </div> -->
    <!--/ Matting. -->

    <!-- Animation. -->
    <!-- <div class="columns is-centered"> -->
    <!--   <div class="column is-full-width"> -->
    <!--     <h2 class="title is-3">Animation</h2> -->

        <!-- Interpolating. -->
        <!-- <h3 class="title is-4">Interpolating states</h3> -->
        <!-- <div class="content has-text-justified"> -->
        <!--   <p> -->
        <!--     We can also animate the scene by interpolating the deformation latent codes of two input -->
        <!--     frames. Use the slider here to linearly interpolate between the left frame and the right -->
        <!--     frame. -->
        <!--   </p> -->
        <!-- </div> -->
        <!-- <div class="columns is-vcentered interpolation-panel"> -->
        <!--   <div class="column is-3 has-text-centered"> -->
        <!--     <img src="./static/images/interpolate_start.jpg" -->
        <!--          class="interpolation-image" -->
        <!--          alt="Interpolate start reference image."/> -->
        <!--     <p>Start Frame</p> -->
        <!--   </div> -->
        <!--   <div class="column interpolation-video-column"> -->
        <!--     <div id="interpolation-image-wrapper"> -->
        <!--       Loading... -->
        <!--     </div> -->
        <!--     <input class="slider is-fullwidth is-large is-info" -->
        <!--            id="interpolation-slider" -->
        <!--            step="1" min="0" max="100" value="0" type="range"> -->
        <!--   </div> -->
        <!--   <div class="column is-3 has-text-centered"> -->
        <!--     <img src="./static/images/interpolate_end.jpg" -->
        <!--          class="interpolation-image" -->
        <!--          alt="Interpolation end reference image."/> -->
        <!--     <p class="is-bold">End Frame</p> -->
        <!--   </div> -->
        <!-- </div> -->
        <!-- <br/> -->
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <!-- <h3 class="title is-4">Re-rendering the input video</h3> -->
        <!-- <div class="content has-text-justified"> -->
        <!--   <p> -->
        <!--     Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel -->
        <!--     viewpoint such as a stabilized camera by playing back the training deformations. -->
        <!--   </p> -->
        <!-- </div> -->
        <!-- <div class="content has-text-centered"> -->
        <!--   <video id="replay-video" -->
        <!--          controls -->
        <!--          muted -->
        <!--          preload -->
        <!--          playsinline -->
        <!--          width="75%"> -->
        <!--     <source src="./static/videos/replay.mp4" -->
        <!--             type="video/mp4"> -->
        <!--   </video> -->
        <!-- </div> -->
        <!--/ Re-rendering. -->

      <!-- </div> -->
    <!-- </div> -->
    <!--/ Animation. -->


    <!-- Concurrent Work. -->
    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            Some stuff about GraphEQA here.
          </p>
        </div>
      </div>
    </div> -->
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{grapheqa2024,
  author    = {Anonymous Author},
  title     = {GraphEQA: Using 3D Semantic Scene Graphs for Real-time Embodied Question Answering},
  journal   = {arXiv},
  year      = {20XX},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This website was built upon the Nerfies website, available at <a
              href="https://github.com/nerfies/nerfies.github.io">this link</a>.

          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
